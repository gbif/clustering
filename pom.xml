<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
  <modelVersion>4.0.0</modelVersion>

  <parent>
    <groupId>org.gbif</groupId>
    <artifactId>motherpom</artifactId>
    <version>58</version>
  </parent>

  <groupId>org.gbif.clustering</groupId>
  <artifactId>clustering-gbif</artifactId>
  <packaging>jar</packaging>
  <version>1.0.0-SNAPSHOT</version>

  <name>GBIF :: Clustering</name>
  <description>Batch process to cluster all GBIF records</description>

  <properties>
    <!-- Common variables -->
    <jdkLevel>11</jdkLevel>
    <maven.compiler.source>${jdkLevel}</maven.compiler.source>
    <maven.compiler.target>${jdkLevel}</maven.compiler.target>

    <encoding>UTF-8</encoding>
    <project.build.sourceEncoding>${encoding}</project.build.sourceEncoding>
    <project.reporting.outputEncoding>${encoding}</project.reporting.outputEncoding>
    <project.resources.sourceEncoding>${encoding}</project.resources.sourceEncoding>

    <!-- Tools-->
    <lombok.version>1.18.28</lombok.version>

    <!-- Spark -->
    <hadoop.version>3.2.4</hadoop.version>
    <hbase.version>2.5.3-hadoop3</hbase.version>
    <curator.version>5.6.0</curator.version>
    <spark.version>3.5.2</spark.version>
    <iceberg.version>1.4.3</iceberg.version>

    <scala.version>2.12.17</scala.version>

    <!-- These must be compatible with Spark version (see https://stackoverflow.com/a/50114083) -->
    <janino.version>3.1.2</janino.version>
    <commons-compiler.version>3.1.2</commons-compiler.version>
    <resilience4j.version>1.7.0</resilience4j.version>

    <junit4.version>4.13.2</junit4.version>

    <maven-scala-plugin.version>4.8.0</maven-scala-plugin.version>
    <googleJavaFormat.version>1.17.0</googleJavaFormat.version>

    <sonar.skip>true</sonar.skip>
  </properties>

  <dependencies>

    <dependency>
      <groupId>org.projectlombok</groupId>
      <artifactId>lombok</artifactId>
      <version>${lombok.version}</version>
      <scope>provided</scope>
    </dependency>

    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-core_2.12</artifactId>
      <version>${spark.version}</version>
      <scope>provided</scope>
    </dependency>
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-sql_2.12</artifactId>
      <version>${spark.version}</version>
      <scope>provided</scope>
    </dependency>
    <dependency>
      <groupId>org.scala-lang</groupId>
      <artifactId>scala-library</artifactId>
      <version>${scala.version}</version>
      <scope>provided</scope>
    </dependency>

    <dependency>
      <groupId>org.apache.iceberg</groupId>
      <artifactId>iceberg-spark-runtime-3.4_2.12</artifactId>
      <version>${iceberg.version}</version>
      <scope>runtime</scope>
    </dependency>

    <dependency>
      <groupId>org.apache.hbase</groupId>
      <artifactId>hbase-client</artifactId>
      <version>${hbase.version}</version>
      <exclusions>
        <!-- Stops the MRAppMaster in Oozie from even starting (when in Oozie) -->
        <exclusion>
          <groupId>org.apache.hadoop</groupId>
          <artifactId>*</artifactId>
        </exclusion>
        <!-- Breaks spark2 -->
        <exclusion>
          <groupId>io.netty</groupId>
          <artifactId>*</artifactId>
        </exclusion>
        <!-- To htrace error -->
        <exclusion>
          <groupId>org.apache.hbase</groupId>
          <artifactId>hbase-hadoop-compat</artifactId>
        </exclusion>
        <exclusion>
          <groupId>org.apache.hbase</groupId>
          <artifactId>hbase-hadoop2-compat</artifactId>
        </exclusion>
      </exclusions>
    </dependency>
    <dependency>
      <groupId>org.apache.hbase</groupId>
      <artifactId>hbase-mapreduce</artifactId>
      <version>${hbase.version}</version>
      <exclusions>
        <exclusion>
          <artifactId>hadoop-mapreduce-client-core</artifactId>
          <groupId>org.apache.hadoop</groupId>
        </exclusion>
        <exclusion>
          <artifactId>hadoop-hdfs-client</artifactId>
          <groupId>org.apache.hadoop</groupId>
        </exclusion>
      </exclusions>
    </dependency>
    <dependency>
      <groupId>org.apache.hbase</groupId>
      <artifactId>hbase-server</artifactId>
      <version>${hbase.version}</version>
      <exclusions>
        <!-- stops the MRAppMaster in Oozie from even starting (when in Oozie) -->
        <exclusion>
          <groupId>org.apache.hadoop</groupId>
          <artifactId>*</artifactId>
        </exclusion>
        <!-- unnecessary bloat -->
        <exclusion>
          <groupId>org.mortbay.jetty</groupId>
          <artifactId>*</artifactId>
        </exclusion>
        <!-- Breaks spark2 -->
        <exclusion>
          <groupId>io.netty</groupId>
          <artifactId>*</artifactId>
        </exclusion>
        <exclusion>
          <groupId>org.codehaus.jackson</groupId>
          <artifactId>jackson-core-asl</artifactId>
        </exclusion>
        <exclusion>
          <groupId>org.codehaus.jackson</groupId>
          <artifactId>jackson-mapper-asl</artifactId>
        </exclusion>
        <exclusion>
          <groupId>org.codehaus.jackson</groupId>
          <artifactId>jackson-jaxrs</artifactId>
        </exclusion>
      </exclusions>
    </dependency>
    <dependency>
      <groupId>org.apache.hadoop</groupId>
      <artifactId>hadoop-client</artifactId>
      <version>${hadoop.version}</version>
      <scope>provided</scope>
      <exclusions>
        <exclusion>
          <groupId>org.codehaus.jackson</groupId>
          <artifactId>jackson-mapper-asl</artifactId>
        </exclusion>
      </exclusions>
    </dependency>

    <!-- These needed for embedded Spark tests https://stackoverflow.com/a/50114083 -->
    <dependency>
      <groupId>org.codehaus.janino</groupId>
      <artifactId>commons-compiler</artifactId>
      <version>${commons-compiler.version}</version>
      <scope>test</scope>
    </dependency>
    <dependency>
      <groupId>org.codehaus.janino</groupId>
      <artifactId>janino</artifactId>
      <version>${janino.version}</version>
      <scope>test</scope>
    </dependency>

    <dependency>
      <groupId>io.github.resilience4j</groupId>
      <artifactId>resilience4j-retry</artifactId>
      <version>${resilience4j.version}</version>
    </dependency>

    <dependency>
      <groupId>junit</groupId>
      <artifactId>junit</artifactId>
      <version>${junit4.version}</version>
    </dependency>
  </dependencies>


  <build>
    <pluginManagement>
      <plugins>
        <plugin>
          <groupId>com.diffplug.spotless</groupId>
          <artifactId>spotless-maven-plugin</artifactId>
          <version>${spotless-maven-plugin.version}</version>
          <configuration combine.self="override">
            <java>
              <googleJavaFormat>
                <version>${googleJavaFormat.version}</version>
                <style>GOOGLE</style>
              </googleJavaFormat>
            </java>
          </configuration>
        </plugin>
        <plugin>
          <groupId>org.apache.maven.plugins</groupId>
          <artifactId>maven-plugin-plugin</artifactId>
          <version>${maven-plugin-plugin.version}</version>
        </plugin>
      </plugins>
    </pluginManagement>
    <plugins>
      <plugin>
        <groupId>com.diffplug.spotless</groupId>
        <artifactId>spotless-maven-plugin</artifactId>
        <executions>
          <execution>
            <!-- Runs in compile phase to fail fast in case of formatting issues.-->
            <id>spotless-check</id>
            <phase>compile</phase>
            <goals>
              <goal>check</goal>
            </goals>
          </execution>
        </executions>
      </plugin>

      <!-- Shade the project into an uber jar to send to Spark -->
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-shade-plugin</artifactId>
        <configuration>
          <createDependencyReducedPom>false</createDependencyReducedPom>
          <keepDependenciesWithProvidedScope>false</keepDependenciesWithProvidedScope>
          <shadedArtifactAttached>true</shadedArtifactAttached>
          <shadedClassifierName>shaded</shadedClassifierName>
          <!-- Breaks relocations otherwise -->
          <shadeTestJar>false</shadeTestJar>
          <filters>
            <filter>
              <artifact>*:*</artifact>
              <excludes>
                <exclude>META-INF/*.SF</exclude>
                <exclude>META-INF/*.DSA</exclude>
                <exclude>META-INF/*.RSA</exclude>
                <exclude>core-site.xml</exclude>
                <exclude>hbase-site.xml</exclude>
                <exclude>**/config-default.xml</exclude>
              </excludes>
            </filter>
          </filters>
        </configuration>
        <executions>
          <execution>
            <phase>package</phase>
            <goals>
              <goal>shade</goal>
            </goals>
            <configuration>
              <transformers>
                <transformer implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
              </transformers>
            </configuration>
          </execution>
        </executions>
      </plugin>
    </plugins>
  </build>

</project>
